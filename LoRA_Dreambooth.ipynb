{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Clone repo and install dependencies"
      ],
      "metadata": {
        "id": "6kUCy2ktbjme"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsBM1XVbvXpv",
        "outputId": "4f4cf384-5950-4226-d06d-7e10199a7996"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lora'...\n",
            "remote: Enumerating objects: 934, done.\u001b[K\n",
            "remote: Counting objects: 100% (473/473), done.\u001b[K\n",
            "remote: Compressing objects: 100% (178/178), done.\u001b[K\n",
            "remote: Total 934 (delta 341), reused 373 (delta 295), pack-reused 461\u001b[K\n",
            "Receiving objects: 100% (934/934), 182.98 MiB | 38.82 MiB/s, done.\n",
            "Resolving deltas: 100% (549/549), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing ./lora\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting diffusers>=0.11.0\n",
            "  Downloading diffusers-0.14.0-py3-none-any.whl (737 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m737.4/737.4 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers>=4.25.1\n",
            "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from lora-diffusion==0.1.7) (1.10.1)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fire\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.14.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors\n",
            "  Downloading safetensors-0.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.9/dist-packages (from lora-diffusion==0.1.7) (4.7.0.72)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from lora-diffusion==0.1.7) (0.15.1+cu118)\n",
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.9.2.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.6/33.6 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.9/dist-packages (from diffusers>=0.11.0->lora-diffusion==0.1.7) (6.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from diffusers>=0.11.0->lora-diffusion==0.1.7) (3.10.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from diffusers>=0.11.0->lora-diffusion==0.1.7) (2.27.1)\n",
            "Collecting huggingface-hub>=0.10.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from diffusers>=0.11.0->lora-diffusion==0.1.7) (8.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from diffusers>=0.11.0->lora-diffusion==0.1.7) (2022.10.31)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from diffusers>=0.11.0->lora-diffusion==0.1.7) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.25.1->lora-diffusion==0.1.7) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.25.1->lora-diffusion==0.1.7) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.25.1->lora-diffusion==0.1.7) (4.65.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from fire->lora-diffusion==0.1.7) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.9/dist-packages (from fire->lora-diffusion==0.1.7) (2.2.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.9/dist-packages (from ftfy->lora-diffusion==0.1.7) (0.2.6)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.9/dist-packages (from mediapipe->lora-diffusion==0.1.7) (22.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from mediapipe->lora-diffusion==0.1.7) (23.3.3)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.9/dist-packages (from mediapipe->lora-diffusion==0.1.7) (4.7.0.72)\n",
            "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.9/dist-packages (from mediapipe->lora-diffusion==0.1.7) (3.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from mediapipe->lora-diffusion==0.1.7) (3.7.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from mediapipe->lora-diffusion==0.1.7) (1.4.0)\n",
            "Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->lora-diffusion==0.1.7) (2.0.0+cu118)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0->torchvision->lora-diffusion==0.1.7) (4.5.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0->torchvision->lora-diffusion==0.1.7) (2.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0->torchvision->lora-diffusion==0.1.7) (3.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0->torchvision->lora-diffusion==0.1.7) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0->torchvision->lora-diffusion==0.1.7) (3.1.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch==2.0.0->torchvision->lora-diffusion==0.1.7) (16.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch==2.0.0->torchvision->lora-diffusion==0.1.7) (3.25.2)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from wandb->lora-diffusion==0.1.7) (1.4.4)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.19.1-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb->lora-diffusion==0.1.7) (67.6.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb->lora-diffusion==0.1.7) (5.9.4)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb->lora-diffusion==0.1.7) (8.1.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->diffusers>=0.11.0->lora-diffusion==0.1.7) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->diffusers>=0.11.0->lora-diffusion==0.1.7) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->diffusers>=0.11.0->lora-diffusion==0.1.7) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->diffusers>=0.11.0->lora-diffusion==0.1.7) (2.0.12)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata->diffusers>=0.11.0->lora-diffusion==0.1.7) (3.15.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe->lora-diffusion==0.1.7) (5.12.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe->lora-diffusion==0.1.7) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe->lora-diffusion==0.1.7) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe->lora-diffusion==0.1.7) (4.39.3)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe->lora-diffusion==0.1.7) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe->lora-diffusion==0.1.7) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->mediapipe->lora-diffusion==0.1.7) (1.4.4)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch==2.0.0->torchvision->lora-diffusion==0.1.7) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch==2.0.0->torchvision->lora-diffusion==0.1.7) (1.3.0)\n",
            "Building wheels for collected packages: lora-diffusion, fire, pathtools\n",
            "  Building wheel for lora-diffusion (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lora-diffusion: filename=lora_diffusion-0.1.7-py3-none-any.whl size=38011 sha256=6ab8c9587b28265a9335729dd012a57f1ae60efc792a0847846557c3e48650c8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-z9ft0wbn/wheels/7f/78/69/42de999ae5ecd88ecdeefcf0d2d3db47de9503bfc79e633b74\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116952 sha256=2a0b6b527b1491e8006bbe399d697f3a4fa6f60602c33aa3345eb196ddbd7142\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/f1/89/b9ea2bf8f80ec027a88fef1d354b3816b4d3d29530988972f6\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=874fb4c54929993daaf14c8647175514df2c89a374f034fef089f74acc46e355\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
            "Successfully built lora-diffusion fire pathtools\n",
            "Installing collected packages: tokenizers, safetensors, pathtools, smmap, setproctitle, sentry-sdk, ftfy, fire, docker-pycreds, huggingface-hub, gitdb, transformers, mediapipe, GitPython, diffusers, wandb, lora-diffusion\n",
            "Successfully installed GitPython-3.1.31 diffusers-0.14.0 docker-pycreds-0.4.0 fire-0.5.0 ftfy-6.1.1 gitdb-4.0.10 huggingface-hub-0.13.4 lora-diffusion-0.1.7 mediapipe-0.9.2.1 pathtools-0.1.2 safetensors-0.3.0 sentry-sdk-1.19.1 setproctitle-1.3.2 smmap-5.0.0 tokenizers-0.13.3 transformers-4.27.4 wandb-0.14.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting cuda-python\n",
            "  Downloading cuda_python-12.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.9/18.9 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.9/dist-packages (from cuda-python) (0.29.34)\n",
            "Installing collected packages: cuda-python\n",
            "Successfully installed cuda-python-12.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting accelerate==0.17.0\n",
            "  Downloading accelerate-0.17.0-py3-none-any.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.8/212.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.37.2-py3-none-any.whl (84.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.2/84.2 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from accelerate==0.17.0) (2.0.0+cu118)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from accelerate==0.17.0) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from accelerate==0.17.0) (23.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from accelerate==0.17.0) (6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate==0.17.0) (5.9.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate==0.17.0) (3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate==0.17.0) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate==0.17.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate==0.17.0) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate==0.17.0) (3.10.7)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate==0.17.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate==0.17.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate==0.17.0) (16.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.4.0->accelerate==0.17.0) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.4.0->accelerate==0.17.0) (1.3.0)\n",
            "Installing collected packages: bitsandbytes, accelerate\n",
            "Successfully installed accelerate-0.17.0 bitsandbytes-0.37.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: cuda-python in /usr/local/lib/python3.9/dist-packages (12.1.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.9/dist-packages (from cuda-python) (0.29.34)\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/HedvigBring/loraRawFiles.git && sed -i 's/functools.cache/functools.lru_cache(maxsize=None)/g' /content/lora/lora_diffusion/xformers_utils.py && pip install /content/lora\n",
        "!pip install cuda-python\n",
        "!pip install accelerate==0.17.0 bitsandbytes\n",
        "!pip install cuda-python\n",
        "!nvcc --version\n",
        "# !pip uninstall -y accelerate\n",
        "# !pip install accelerate==0.17.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Drive"
      ],
      "metadata": {
        "id": "5L1ayQcbbOdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i13c6koqbIua",
        "outputId": "15c4a52f-a290-47e3-9c30-d0b4ddda3766"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set directories"
      ],
      "metadata": {
        "id": "W5LMQ8C0bRDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "from tqdm import tqdm\n",
        "\n",
        "PRETRAINED_MODEL=\"runwayml/stable-diffusion-v1-5\" #@param{type: 'string'}\n",
        "PROMPT=\"erosion\" #@param{type: 'string'}\n",
        "\n",
        "OUTPUT_DIR=\"/content/drive/MyDrive/ErosionJpg/Ouputs3\" #@param{type: 'string'}\n",
        "IMAGES_FOLDER_OPTIONAL=\"/content/drive/MyDrive/ErosionJpg/10_Erosion\" #@param{type: 'string'}\n",
        "\n",
        "RESOLUTION=\"512\" #@param [\"512\", \"576\", \"640\", \"704\", \"768\", \"832\", \"896\", \"960\", \"1024\"]\n",
        "RESOLUTION=int(RESOLUTION)\n",
        "\n",
        "if PRETRAINED_MODEL == \"\":\n",
        "  print('\u001b[1;31mYou should define the pretrained model.')\n",
        "\n",
        "else:\n",
        "  if IMAGES_FOLDER_OPTIONAL==\"\":\n",
        "    INSTANCE_DIR = \"/content/data_example\"\n",
        "    if not os.path.exists(str(INSTANCE_DIR)):\n",
        "      %mkdir -p \"$INSTANCE_DIR\"\n",
        "    uploaded = files.upload()\n",
        "    for filename in tqdm(uploaded.keys(), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n",
        "        shutil.move(filename, INSTANCE_DIR)\n",
        "  else:\n",
        "    INSTANCE_DIR = IMAGES_FOLDER_OPTIONAL\n",
        "  \n",
        "  if OUTPUT_DIR == \"\":\n",
        "    OUTPUT_DIR = \"/content/output\"\n",
        "  if not os.path.exists(str(OUTPUT_DIR)):\n",
        "    %mkdir -p \"$OUTPUT_DIR\""
      ],
      "metadata": {
        "id": "1aX8JJPTbOHu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run training"
      ],
      "metadata": {
        "id": "lmAu11YQbXdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STEPS = 10000 #@param {type:\"slider\", min:0, max:10000, step:10}\n",
        "BATCH_SIZE = 6 #@param {type:\"slider\", min:0, max:128, step:1}\n",
        "FP_16 = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ----\n",
        "#@markdown UNET PARAMS\n",
        "LEARNING_RATE = 1e-4 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown ----\n",
        "TRAIN_TEXT_ENCODER = True #@param {type:\"boolean\"}\n",
        "#@markdown TEXT ENCODER PARAMS\n",
        "LEARNING_RATE_TEXT_ENCODER = 5e-5 #@param {type:\"number\"}\n",
        "\n",
        "NEW_LEARNING_RATE = LEARNING_RATE / BATCH_SIZE\n",
        "NEW_LEARNING_RATE_TEXT_ENCODER = LEARNING_RATE_TEXT_ENCODER / BATCH_SIZE\n",
        "\n",
        "if FP_16:\n",
        "  fp_16_arg = \"fp16\"\n",
        "else:\n",
        "  fp_16_arg = \"no\"\n",
        "\n",
        "# train_command = f\"\"\"accelerate launch lora/training_scripts/train_lora_dreambooth.py \\\n",
        "#              --pretrained_model_name_or_path=\"{PRETRAINED_MODEL}\" \\\n",
        "#              --instance_data_dir=\"{INSTANCE_DIR}\" \\\n",
        "#              --output_dir=\"{OUTPUT_DIR}\" \\\n",
        "#              --instance_prompt=\"{PROMPT}\" \\\n",
        "#              --resolution=512 \\\n",
        "#              --use_8bit_adam \\\n",
        "#              --mixed_precision=\"{fp_16_arg}\" \\\n",
        "#              --train_batch_size={BATCH_SIZE} \\\n",
        "#              --gradient_accumulation_steps=1 \\\n",
        "#              --learning_rate={NEW_LEARNING_RATE} \\\n",
        "#              --lr_scheduler=\"constant\" \\\n",
        "#              --lr_warmup_steps=0 \\\n",
        "#              --max_train_steps={STEPS} \\\n",
        "#              --train_text_encoder \\\n",
        "#              --lora_rank=16 \\\n",
        "#              --learning_rate_text={NEW_LEARNING_RATE_TEXT_ENCODER}\"\"\"\n",
        "\n",
        "# print(train_command)\n",
        "# f = open(\"./train.sh\", \"w\")\n",
        "# f.write(train_command)\n",
        "# f.close()\n",
        "# !chmod +x ./train.sh\n",
        "# !./train.sh\n",
        "\n",
        "if TRAIN_TEXT_ENCODER:\n",
        "  command = (f'/usr/bin/python3 lora/training_scripts/train_lora_dreambooth.py '\n",
        "             f'--pretrained_model_name_or_path=\"{PRETRAINED_MODEL}\" '\n",
        "             f'--instance_data_dir=\"{INSTANCE_DIR}\" '\n",
        "             f'--output_dir=\"{OUTPUT_DIR}\" '\n",
        "             f'--instance_prompt=\"{PROMPT}\" '\n",
        "             f'--resolution=512 '\n",
        "             f'--mixed_precision=\"{fp_16_arg}\" '\n",
        "             f'--train_batch_size={BATCH_SIZE} '\n",
        "             f'--gradient_accumulation_steps=1 '\n",
        "             f'--learning_rate={NEW_LEARNING_RATE} '\n",
        "             f'--lr_scheduler=\"constant\" '\n",
        "             f'--lr_warmup_steps=0 '\n",
        "             f'--max_train_steps={STEPS} '\n",
        "             f'--train_text_encoder '\n",
        "             f'--lora_rank=16 '\n",
        "             f'--learning_rate_text={NEW_LEARNING_RATE_TEXT_ENCODER}')\n",
        "else:\n",
        "  command = (f'accelerate launch lora/training_scripts/train_lora_dreambooth.py '\n",
        "             f'--pretrained_model_name_or_path=\"{PRETRAINED_MODEL}\" '\n",
        "             f'--instance_data_dir=\"{INSTANCE_DIR}\" '\n",
        "             f'--output_dir=\"{OUTPUT_DIR}\" '\n",
        "             f'--instance_prompt=\"{PROMPT}\" '\n",
        "             f'--resolution=512 '\n",
        "             f'--use_8bit_adam '\n",
        "             f'--mixed_precision=\"{fp_16_arg}\" '\n",
        "             f'--train_batch_size=1 '\n",
        "             f'--gradient_accumulation_steps={BATCH_SIZE} '\n",
        "             f'--learning_rate={NEW_LEARNING_RATE} '\n",
        "             f'--lr_scheduler=\"constant\" '\n",
        "             f'--lr_warmup_steps=0 '\n",
        "             f'--lora_rank=16 '\n",
        "             f'--max_train_steps={STEPS} '\n",
        "             f'--learning_rate_text={NEW_LEARNING_RATE_TEXT_ENCODER}')\n",
        "!rm -rf $INSTANCE_DIR/.ipynb_checkpoints\n",
        "!{command}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1Zlrn58bXzi",
        "outputId": "428dfdb5-6e55-42cb-d03b-81705cb857c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-10 17:35:41.496448: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.9/dist-packages/accelerate/accelerator.py:243: FutureWarning: `logging_dir` is deprecated and will be removed in version 0.18.0 of 🤗 Accelerate. Use `project_dir` instead.\n",
            "  warnings.warn(\n",
            "Downloading (…)tokenizer/vocab.json: 100% 1.06M/1.06M [00:00<00:00, 13.3MB/s]\n",
            "Downloading (…)tokenizer/merges.txt: 100% 525k/525k [00:00<00:00, 8.85MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 472/472 [00:00<00:00, 172kB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 806/806 [00:00<00:00, 316kB/s]\n",
            "Downloading (…)_encoder/config.json: 100% 617/617 [00:00<00:00, 84.4kB/s]\n",
            "Downloading model.safetensors: 100% 492M/492M [00:03<00:00, 161MB/s]\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/modeling_utils.py:402: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  with safe_open(checkpoint_file, framework=\"pt\") as f:\n",
            "/usr/local/lib/python3.9/dist-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n",
            "/usr/local/lib/python3.9/dist-packages/torch/storage.py:899: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  storage = cls(wrap_storage=untyped_storage)\n",
            "/usr/local/lib/python3.9/dist-packages/safetensors/torch.py:99: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  with safe_open(filename, framework=\"pt\", device=device) as f:\n",
            "Downloading (…)ch_model.safetensors: 100% 335M/335M [00:01<00:00, 200MB/s]\n",
            "Downloading (…)main/vae/config.json: 100% 547/547 [00:00<00:00, 96.0kB/s]\n",
            "Downloading (…)ch_model.safetensors: 100% 3.44G/3.44G [00:27<00:00, 125MB/s]\n",
            "Downloading (…)ain/unet/config.json: 100% 743/743 [00:00<00:00, 103kB/s]\n",
            "Before training: Unet First Layer lora up tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "Before training: Unet First Layer lora down tensor([[ 0.0093, -0.0370,  0.0425,  ..., -0.0085,  0.0748, -0.0268],\n",
            "        [ 0.0350, -0.0609, -0.0949,  ...,  0.0296,  0.0744,  0.1008],\n",
            "        [ 0.0650, -0.0302, -0.0994,  ..., -0.0402,  0.0216,  0.0571],\n",
            "        ...,\n",
            "        [ 0.0985, -0.0602, -0.0108,  ...,  0.0649,  0.0116,  0.0099],\n",
            "        [ 0.0882, -0.0660, -0.0108,  ...,  0.0771, -0.1071, -0.1630],\n",
            "        [-0.0382,  0.0783,  0.0138,  ...,  0.0879,  0.0068, -0.0349]])\n",
            "Before training: text encoder First Layer lora up tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "Before training: text encoder First Layer lora down tensor([[-0.1379, -0.0145, -0.0660,  ...,  0.0029, -0.0783, -0.0186],\n",
            "        [-0.0534,  0.0136,  0.0247,  ...,  0.0567, -0.0588,  0.1310],\n",
            "        [-0.0274,  0.0659, -0.0115,  ...,  0.0942,  0.0098, -0.0118],\n",
            "        ...,\n",
            "        [ 0.0726, -0.0542, -0.1520,  ...,  0.0680,  0.0184,  0.0185],\n",
            "        [-0.0715,  0.0260, -0.0328,  ..., -0.0185,  0.0267,  0.0141],\n",
            "        [-0.0312, -0.0271, -0.0062,  ...,  0.0121,  0.0709,  0.0146]])\n",
            "/usr/local/lib/python3.9/dist-packages/diffusers/configuration_utils.py:195: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a scheduler, please use <class 'diffusers.schedulers.scheduling_ddpm.DDPMScheduler'>.from_pretrained(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n",
            "  deprecate(\"config-passed-as-path\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
            "Downloading (…)cheduler_config.json: 100% 308/308 [00:00<00:00, 83.7kB/s]\n",
            "***** Running training *****\n",
            "  Num examples = 20\n",
            "  Num batches each epoch = 4\n",
            "  Num Epochs = 2500\n",
            "  Instantaneous batch size per device = 6\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 6\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 10000\n",
            "Steps:   5% 500/10000 [16:48<4:36:17,  1.75s/it, loss=0.0806, lr=1.67e-5]\n",
            "Downloading (…)ain/model_index.json: 100% 543/543 [00:00<00:00, 79.3kB/s]\n",
            "\n",
            "Fetching 15 files:   0% 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   0% 0.00/1.22G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)_checker/config.json:   0% 0.00/4.72k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)_checker/config.json: 100% 4.72k/4.72k [00:00<00:00, 387kB/s]\n",
            "Downloading (…)rocessor_config.json: 100% 342/342 [00:00<00:00, 50.8kB/s]\n",
            "\n",
            "Fetching 15 files:   7% 1/15 [00:00<00:02,  5.67it/s]\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   1% 10.5M/1.22G [00:00<00:17, 70.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   2% 21.0M/1.22G [00:00<00:15, 78.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   3% 31.5M/1.22G [00:00<00:14, 81.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   3% 41.9M/1.22G [00:00<00:15, 75.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   4% 52.4M/1.22G [00:00<00:15, 75.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   5% 62.9M/1.22G [00:00<00:15, 76.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   6% 73.4M/1.22G [00:00<00:14, 78.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   7% 83.9M/1.22G [00:01<00:14, 79.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   8% 94.4M/1.22G [00:01<00:14, 79.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   9% 105M/1.22G [00:01<00:14, 77.0MB/s] \u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   9% 115M/1.22G [00:01<00:14, 78.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  10% 126M/1.22G [00:01<00:15, 70.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  11% 136M/1.22G [00:01<00:14, 74.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  12% 147M/1.22G [00:01<00:13, 77.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  13% 157M/1.22G [00:02<00:14, 71.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  14% 168M/1.22G [00:02<00:14, 73.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  15% 178M/1.22G [00:02<00:14, 73.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  16% 189M/1.22G [00:02<00:13, 75.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  16% 199M/1.22G [00:02<00:13, 78.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  17% 210M/1.22G [00:02<00:12, 79.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  18% 220M/1.22G [00:02<00:12, 81.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  19% 231M/1.22G [00:02<00:12, 80.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  20% 241M/1.22G [00:03<00:12, 80.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  21% 252M/1.22G [00:03<00:12, 79.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  22% 262M/1.22G [00:03<00:12, 76.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  22% 273M/1.22G [00:03<00:12, 73.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  23% 283M/1.22G [00:03<00:14, 65.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  24% 294M/1.22G [00:03<00:13, 68.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  25% 304M/1.22G [00:04<00:12, 73.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  26% 315M/1.22G [00:04<00:12, 74.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  27% 325M/1.22G [00:04<00:11, 77.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  28% 336M/1.22G [00:04<00:13, 66.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  28% 346M/1.22G [00:04<00:12, 69.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  29% 357M/1.22G [00:04<00:11, 73.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  30% 367M/1.22G [00:04<00:11, 72.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  31% 377M/1.22G [00:05<00:11, 72.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  32% 388M/1.22G [00:05<00:11, 74.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  33% 398M/1.22G [00:05<00:11, 74.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  34% 409M/1.22G [00:05<00:10, 76.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  34% 419M/1.22G [00:05<00:10, 75.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  35% 430M/1.22G [00:05<00:10, 77.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  36% 440M/1.22G [00:05<00:09, 79.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  37% 451M/1.22G [00:05<00:09, 78.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  38% 461M/1.22G [00:06<00:09, 75.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  39% 472M/1.22G [00:06<00:09, 77.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  40% 482M/1.22G [00:06<00:09, 74.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  41% 493M/1.22G [00:06<00:09, 76.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  41% 503M/1.22G [00:06<00:09, 78.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  42% 514M/1.22G [00:06<00:09, 77.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  43% 524M/1.22G [00:06<00:09, 74.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  44% 535M/1.22G [00:07<00:09, 73.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  45% 545M/1.22G [00:07<00:08, 75.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  46% 556M/1.22G [00:07<00:08, 76.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  47% 566M/1.22G [00:07<00:08, 75.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  47% 577M/1.22G [00:07<00:09, 70.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  48% 587M/1.22G [00:07<00:09, 68.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  49% 598M/1.22G [00:08<00:12, 48.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  50% 608M/1.22G [00:08<00:10, 55.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  51% 619M/1.22G [00:08<00:09, 61.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  52% 629M/1.22G [00:08<00:08, 65.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  53% 640M/1.22G [00:08<00:08, 64.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  53% 650M/1.22G [00:08<00:08, 68.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  54% 661M/1.22G [00:09<00:07, 73.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  55% 671M/1.22G [00:09<00:07, 76.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  56% 682M/1.22G [00:09<00:07, 72.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  57% 692M/1.22G [00:09<00:06, 76.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  58% 703M/1.22G [00:09<00:06, 78.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  59% 713M/1.22G [00:09<00:06, 77.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  60% 724M/1.22G [00:09<00:06, 78.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  60% 734M/1.22G [00:09<00:06, 80.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  61% 744M/1.22G [00:10<00:06, 76.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  62% 755M/1.22G [00:10<00:05, 78.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  63% 765M/1.22G [00:10<00:05, 75.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  64% 776M/1.22G [00:10<00:05, 76.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  65% 786M/1.22G [00:10<00:05, 76.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  66% 807M/1.22G [00:10<00:04, 82.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  67% 818M/1.22G [00:12<00:17, 22.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  68% 828M/1.22G [00:12<00:13, 28.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  70% 849M/1.22G [00:12<00:08, 42.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  71% 860M/1.22G [00:12<00:07, 47.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  72% 870M/1.22G [00:12<00:06, 52.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  72% 881M/1.22G [00:13<00:05, 58.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  73% 891M/1.22G [00:13<00:05, 63.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  74% 902M/1.22G [00:13<00:04, 63.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  75% 912M/1.22G [00:13<00:04, 66.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  76% 923M/1.22G [00:13<00:04, 67.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  77% 933M/1.22G [00:13<00:03, 71.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  78% 944M/1.22G [00:13<00:03, 73.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  78% 954M/1.22G [00:14<00:03, 74.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  79% 965M/1.22G [00:14<00:03, 77.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  80% 975M/1.22G [00:14<00:03, 79.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  81% 986M/1.22G [00:14<00:03, 76.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  82% 996M/1.22G [00:14<00:03, 71.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  83% 1.01G/1.22G [00:14<00:02, 73.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  84% 1.02G/1.22G [00:14<00:03, 63.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  85% 1.03G/1.22G [00:15<00:02, 69.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  85% 1.04G/1.22G [00:15<00:02, 65.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  86% 1.05G/1.22G [00:15<00:02, 70.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  87% 1.06G/1.22G [00:15<00:02, 70.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  88% 1.07G/1.22G [00:15<00:01, 74.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  89% 1.08G/1.22G [00:15<00:01, 73.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  90% 1.09G/1.22G [00:15<00:01, 74.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  91% 1.10G/1.22G [00:16<00:01, 75.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  91% 1.11G/1.22G [00:16<00:01, 76.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  92% 1.12G/1.22G [00:16<00:01, 78.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  93% 1.13G/1.22G [00:16<00:01, 73.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  94% 1.14G/1.22G [00:16<00:01, 71.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  95% 1.15G/1.22G [00:16<00:00, 68.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  96% 1.16G/1.22G [00:16<00:00, 71.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  97% 1.17G/1.22G [00:17<00:00, 73.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  97% 1.18G/1.22G [00:17<00:00, 75.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  98% 1.20G/1.22G [00:17<00:00, 74.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors: 100% 1.22G/1.22G [00:17<00:00, 68.8MB/s]\n",
            "\n",
            "Fetching 15 files: 100% 15/15 [00:17<00:00,  1.19s/it]\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
            "  warnings.warn(\n",
            "save weights /content/drive/MyDrive/ErosionJpg/Ouputs3/lora_weight_e124_s500.pt, /content/drive/MyDrive/ErosionJpg/Ouputs3/lora_weight_e124_s500.text_encoder.pt\n",
            "First Unet Layer's Up Weight is now :  tensor([[ 4.5775e-04,  8.0600e-05,  6.5741e-04,  ...,  1.0098e-03,\n",
            "          9.0460e-04,  6.4569e-04],\n",
            "        [-1.0582e-03,  1.1313e-03,  1.0897e-04,  ..., -3.0841e-04,\n",
            "          1.0844e-03,  6.8834e-04],\n",
            "        [ 8.3198e-04,  5.7899e-04,  9.2204e-04,  ...,  1.4107e-04,\n",
            "         -1.1664e-04, -8.5551e-04],\n",
            "        ...,\n",
            "        [ 2.3442e-04, -2.0393e-03, -2.6578e-04,  ...,  1.1624e-03,\n",
            "         -1.8958e-03, -1.0140e-03],\n",
            "        [-9.7222e-04, -7.1180e-04,  9.1123e-04,  ...,  7.1176e-05,\n",
            "          5.4817e-04,  7.5459e-04],\n",
            "        [ 1.4230e-03,  1.2956e-03, -1.5555e-03,  ..., -2.6956e-04,\n",
            "          1.0424e-03, -1.1628e-03]], device='cuda:0')\n",
            "First Unet Layer's Down Weight is now :  tensor([[ 0.0095, -0.0366,  0.0437,  ..., -0.0089,  0.0778, -0.0284],\n",
            "        [ 0.0349, -0.0626, -0.0954,  ...,  0.0289,  0.0753,  0.1000],\n",
            "        [ 0.0657, -0.0308, -0.1006,  ..., -0.0400,  0.0201,  0.0573],\n",
            "        ...,\n",
            "        [ 0.0999, -0.0599, -0.0096,  ...,  0.0648,  0.0137,  0.0080],\n",
            "        [ 0.0882, -0.0671, -0.0120,  ...,  0.0787, -0.1087, -0.1633],\n",
            "        [-0.0381,  0.0778,  0.0128,  ...,  0.0884,  0.0045, -0.0345]],\n",
            "       device='cuda:0')\n",
            "First Text Encoder Layer's Up Weight is now :  tensor([[ 2.4149e-04, -4.5574e-04, -2.8099e-04,  ..., -4.2525e-04,\n",
            "          7.5344e-05,  4.8219e-04],\n",
            "        [-2.6706e-04,  4.7964e-04,  3.0969e-04,  ...,  4.5505e-04,\n",
            "         -5.7471e-05, -4.9558e-04],\n",
            "        [-1.9037e-04,  4.2434e-04,  2.3033e-04,  ...,  3.8240e-04,\n",
            "         -1.0514e-04, -4.4961e-04],\n",
            "        ...,\n",
            "        [ 1.0214e-03, -8.6771e-04, -1.0070e-03,  ..., -9.3525e-04,\n",
            "         -1.0134e-03,  7.8594e-04],\n",
            "        [-9.4471e-04,  6.4059e-04,  9.2469e-04,  ...,  7.8738e-04,\n",
            "          1.0522e-03, -3.5997e-04],\n",
            "        [-9.8260e-04,  8.0599e-04,  9.6707e-04,  ...,  8.8310e-04,\n",
            "          1.0345e-03, -6.4179e-04]], device='cuda:0')\n",
            "First Text Encoder Layer's Down Weight is now :  tensor([[-0.1374, -0.0143, -0.0654,  ...,  0.0032, -0.0787, -0.0182],\n",
            "        [-0.0539,  0.0135,  0.0242,  ...,  0.0564, -0.0584,  0.1307],\n",
            "        [-0.0279,  0.0657, -0.0121,  ...,  0.0939,  0.0102, -0.0122],\n",
            "        ...,\n",
            "        [ 0.0721, -0.0542, -0.1525,  ...,  0.0677,  0.0189,  0.0182],\n",
            "        [-0.0719,  0.0258, -0.0332,  ..., -0.0188,  0.0270,  0.0138],\n",
            "        [-0.0308, -0.0271, -0.0057,  ...,  0.0123,  0.0706,  0.0149]],\n",
            "       device='cuda:0')\n",
            "Steps:  10% 1000/10000 [33:49<4:22:32,  1.75s/it, loss=0.114, lr=1.67e-5]\n",
            "Fetching 15 files: 100% 15/15 [00:00<00:00, 58853.66it/s]\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "save weights /content/drive/MyDrive/ErosionJpg/Ouputs3/lora_weight_e249_s1000.pt, /content/drive/MyDrive/ErosionJpg/Ouputs3/lora_weight_e249_s1000.text_encoder.pt\n",
            "First Unet Layer's Up Weight is now :  tensor([[ 3.8127e-04,  3.8949e-04,  1.2019e-03,  ...,  7.7098e-04,\n",
            "          2.2218e-03,  9.9764e-04],\n",
            "        [-8.7195e-04,  1.1118e-03,  4.3032e-04,  ...,  1.5516e-04,\n",
            "          1.3477e-03,  3.9916e-04],\n",
            "        [ 1.4291e-03,  1.0195e-03,  1.6045e-03,  ...,  4.8184e-04,\n",
            "          2.9812e-04, -1.0567e-03],\n",
            "        ...,\n",
            "        [-4.1129e-04, -3.4880e-03, -2.8355e-04,  ...,  1.0381e-03,\n",
            "         -3.1684e-03, -1.5799e-03],\n",
            "        [-1.3237e-03, -9.3046e-04,  1.4325e-03,  ..., -1.7987e-05,\n",
            "          1.0443e-03,  1.4291e-03],\n",
            "        [ 2.4100e-03,  1.0736e-03, -2.4616e-03,  ..., -1.6445e-04,\n",
            "          1.1513e-03, -2.3293e-03]], device='cuda:0')\n",
            "First Unet Layer's Down Weight is now :  tensor([[ 0.0104, -0.0363,  0.0434,  ..., -0.0095,  0.0776, -0.0280],\n",
            "        [ 0.0355, -0.0634, -0.0966,  ...,  0.0292,  0.0752,  0.1005],\n",
            "        [ 0.0651, -0.0313, -0.1008,  ..., -0.0392,  0.0205,  0.0567],\n",
            "        ...,\n",
            "        [ 0.1005, -0.0608, -0.0099,  ...,  0.0640,  0.0132,  0.0079],\n",
            "        [ 0.0870, -0.0676, -0.0121,  ...,  0.0810, -0.1076, -0.1641],\n",
            "        [-0.0390,  0.0777,  0.0131,  ...,  0.0894,  0.0046, -0.0352]],\n",
            "       device='cuda:0')\n",
            "First Text Encoder Layer's Up Weight is now :  tensor([[ 4.3375e-04, -6.2593e-04, -4.7655e-04,  ..., -6.1484e-04,\n",
            "         -1.2848e-04,  6.3782e-04],\n",
            "        [-4.4019e-04,  6.1335e-04,  4.8426e-04,  ...,  6.1302e-04,\n",
            "          1.3770e-04, -6.0130e-04],\n",
            "        [-3.6374e-04,  5.6872e-04,  4.0598e-04,  ...,  5.4572e-04,\n",
            "          8.8157e-05, -5.7404e-04],\n",
            "        ...,\n",
            "        [ 9.7739e-04, -7.5642e-04, -9.6048e-04,  ..., -8.5447e-04,\n",
            "         -1.0476e-03,  6.6815e-04],\n",
            "        [-1.0185e-03,  7.1754e-04,  1.0006e-03,  ...,  8.7131e-04,\n",
            "          1.1134e-03, -4.3527e-04],\n",
            "        [-1.0166e-03,  7.9998e-04,  1.0013e-03,  ...,  9.0338e-04,\n",
            "          1.0885e-03, -6.2719e-04]], device='cuda:0')\n",
            "First Text Encoder Layer's Down Weight is now :  tensor([[-0.1373, -0.0143, -0.0654,  ...,  0.0032, -0.0787, -0.0182],\n",
            "        [-0.0539,  0.0135,  0.0241,  ...,  0.0564, -0.0584,  0.1307],\n",
            "        [-0.0279,  0.0657, -0.0121,  ...,  0.0939,  0.0102, -0.0122],\n",
            "        ...,\n",
            "        [ 0.0721, -0.0543, -0.1525,  ...,  0.0677,  0.0189,  0.0182],\n",
            "        [-0.0718,  0.0258, -0.0331,  ..., -0.0187,  0.0270,  0.0139],\n",
            "        [-0.0307, -0.0270, -0.0057,  ...,  0.0124,  0.0705,  0.0149]],\n",
            "       device='cuda:0')\n",
            "Steps:  15% 1500/10000 [50:36<4:03:41,  1.72s/it, loss=0.0679, lr=1.67e-5]\n",
            "Fetching 15 files: 100% 15/15 [00:00<00:00, 127357.41it/s]\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "save weights /content/drive/MyDrive/ErosionJpg/Ouputs3/lora_weight_e374_s1500.pt, /content/drive/MyDrive/ErosionJpg/Ouputs3/lora_weight_e374_s1500.text_encoder.pt\n",
            "First Unet Layer's Up Weight is now :  tensor([[ 5.2490e-04,  9.0704e-05,  1.6201e-03,  ...,  1.0892e-03,\n",
            "          3.1052e-03,  1.3159e-03],\n",
            "        [-3.0758e-04,  4.3757e-04,  4.5685e-04,  ...,  1.0308e-03,\n",
            "          1.3383e-03,  1.5240e-04],\n",
            "        [ 1.8539e-03,  9.0853e-04,  1.9709e-03,  ...,  1.1379e-03,\n",
            "          4.1237e-04, -1.2566e-03],\n",
            "        ...,\n",
            "        [-1.0026e-03, -3.9528e-03,  2.7894e-04,  ...,  7.9192e-04,\n",
            "         -3.6150e-03, -1.7505e-03],\n",
            "        [-1.3924e-03, -7.6188e-04,  1.2585e-03,  ..., -2.1526e-04,\n",
            "          1.1607e-03,  1.7829e-03],\n",
            "        [ 3.1029e-03,  5.7513e-04, -2.9127e-03,  ...,  4.0291e-04,\n",
            "          1.2699e-03, -2.7313e-03]], device='cuda:0')\n",
            "First Unet Layer's Down Weight is now :  tensor([[ 0.0108, -0.0355,  0.0429,  ..., -0.0090,  0.0779, -0.0278],\n",
            "        [ 0.0358, -0.0641, -0.0970,  ...,  0.0297,  0.0760,  0.1008],\n",
            "        [ 0.0649, -0.0323, -0.1004,  ..., -0.0394,  0.0203,  0.0563],\n",
            "        ...,\n",
            "        [ 0.1020, -0.0611, -0.0095,  ...,  0.0638,  0.0128,  0.0076],\n",
            "        [ 0.0857, -0.0685, -0.0119,  ...,  0.0817, -0.1073, -0.1645],\n",
            "        [-0.0398,  0.0770,  0.0136,  ...,  0.0893,  0.0042, -0.0358]],\n",
            "       device='cuda:0')\n",
            "First Text Encoder Layer's Up Weight is now :  tensor([[ 0.0006, -0.0007, -0.0006,  ..., -0.0007, -0.0003,  0.0007],\n",
            "        [-0.0006,  0.0007,  0.0006,  ...,  0.0007,  0.0003, -0.0006],\n",
            "        [-0.0005,  0.0006,  0.0005,  ...,  0.0006,  0.0002, -0.0006],\n",
            "        ...,\n",
            "        [ 0.0009, -0.0007, -0.0009,  ..., -0.0008, -0.0009,  0.0006],\n",
            "        [-0.0009,  0.0007,  0.0009,  ...,  0.0008,  0.0010, -0.0004],\n",
            "        [-0.0009,  0.0007,  0.0009,  ...,  0.0008,  0.0010, -0.0006]],\n",
            "       device='cuda:0')\n",
            "First Text Encoder Layer's Down Weight is now :  tensor([[-0.1373, -0.0144, -0.0654,  ...,  0.0032, -0.0788, -0.0182],\n",
            "        [-0.0539,  0.0135,  0.0241,  ...,  0.0564, -0.0584,  0.1307],\n",
            "        [-0.0280,  0.0658, -0.0121,  ...,  0.0939,  0.0103, -0.0122],\n",
            "        ...,\n",
            "        [ 0.0721, -0.0543, -0.1525,  ...,  0.0677,  0.0189,  0.0182],\n",
            "        [-0.0718,  0.0259, -0.0331,  ..., -0.0187,  0.0270,  0.0139],\n",
            "        [-0.0307, -0.0270, -0.0057,  ...,  0.0124,  0.0705,  0.0149]],\n",
            "       device='cuda:0')\n",
            "Steps:  20% 2000/10000 [1:07:21<3:50:08,  1.73s/it, loss=0.0425, lr=1.67e-5]\n",
            "Fetching 15 files: 100% 15/15 [00:00<00:00, 37946.06it/s]\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "save weights /content/drive/MyDrive/ErosionJpg/Ouputs3/lora_weight_e499_s2000.pt, /content/drive/MyDrive/ErosionJpg/Ouputs3/lora_weight_e499_s2000.text_encoder.pt\n",
            "First Unet Layer's Up Weight is now :  tensor([[ 5.1611e-04,  7.2607e-05,  1.7190e-03,  ...,  1.3616e-03,\n",
            "          3.6093e-03,  1.4541e-03],\n",
            "        [ 3.1166e-04, -1.1168e-04,  4.1403e-04,  ...,  1.6831e-03,\n",
            "          9.7801e-04, -1.6162e-04],\n",
            "        [ 2.1399e-03,  1.0063e-03,  2.1499e-03,  ...,  1.5427e-03,\n",
            "          4.4420e-04, -1.4676e-03],\n",
            "        ...,\n",
            "        [-8.9664e-04, -3.8910e-03,  5.4044e-04,  ...,  4.9769e-04,\n",
            "         -3.9174e-03, -1.7374e-03],\n",
            "        [-1.5190e-03, -5.6469e-04,  1.1380e-03,  ..., -8.2198e-05,\n",
            "          1.3689e-03,  1.9464e-03],\n",
            "        [ 3.6445e-03,  3.6979e-05, -3.1010e-03,  ...,  9.4532e-04,\n",
            "          1.1221e-03, -3.0144e-03]], device='cuda:0')\n",
            "First Unet Layer's Down Weight is now :  tensor([[ 0.0117, -0.0354,  0.0428,  ..., -0.0092,  0.0776, -0.0273],\n",
            "        [ 0.0365, -0.0647, -0.0972,  ...,  0.0300,  0.0766,  0.1001],\n",
            "        [ 0.0648, -0.0326, -0.1005,  ..., -0.0395,  0.0207,  0.0560],\n",
            "        ...,\n",
            "        [ 0.1038, -0.0613, -0.0092,  ...,  0.0633,  0.0128,  0.0077],\n",
            "        [ 0.0845, -0.0688, -0.0123,  ...,  0.0825, -0.1068, -0.1651],\n",
            "        [-0.0409,  0.0770,  0.0137,  ...,  0.0898,  0.0043, -0.0366]],\n",
            "       device='cuda:0')\n",
            "First Text Encoder Layer's Up Weight is now :  tensor([[ 0.0005, -0.0007, -0.0006,  ..., -0.0007, -0.0003,  0.0007],\n",
            "        [-0.0005,  0.0006,  0.0006,  ...,  0.0007,  0.0003, -0.0006],\n",
            "        [-0.0004,  0.0006,  0.0005,  ...,  0.0006,  0.0002, -0.0006],\n",
            "        ...,\n",
            "        [ 0.0009, -0.0007, -0.0008,  ..., -0.0007, -0.0009,  0.0006],\n",
            "        [-0.0009,  0.0006,  0.0009,  ...,  0.0008,  0.0010, -0.0003],\n",
            "        [-0.0009,  0.0007,  0.0009,  ...,  0.0008,  0.0010, -0.0005]],\n",
            "       device='cuda:0')\n",
            "First Text Encoder Layer's Down Weight is now :  tensor([[-0.1374, -0.0145, -0.0655,  ...,  0.0031, -0.0786, -0.0183],\n",
            "        [-0.0538,  0.0135,  0.0243,  ...,  0.0565, -0.0585,  0.1308],\n",
            "        [-0.0278,  0.0658, -0.0120,  ...,  0.0940,  0.0101, -0.0121],\n",
            "        ...,\n",
            "        [ 0.0722, -0.0542, -0.1524,  ...,  0.0678,  0.0188,  0.0183],\n",
            "        [-0.0716,  0.0260, -0.0330,  ..., -0.0186,  0.0268,  0.0140],\n",
            "        [-0.0308, -0.0270, -0.0058,  ...,  0.0123,  0.0706,  0.0149]],\n",
            "       device='cuda:0')\n",
            "Steps:  21% 2129/10000 [1:11:44<4:46:13,  2.18s/it, loss=0.0363, lr=1.67e-5]"
          ]
        }
      ]
    }
  ]
}